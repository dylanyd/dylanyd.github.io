[{"title":"Autonomous driving specialization 课程笔记1","date":"2020-11-13T08:06:48.000Z","url":"/2020/11/13/autonomous-driving-notes-1/","tags":[["course notes","/tags/course-notes/"]],"categories":[["self-driving","/categories/self-driving/"]],"content":"Autonomous driving specialization: Introduction to self-drivingTerms and DefinitionsDriving task Perceiving the environment Planning how to reach from A to B Controlling the vehicle Operational Design Domain (ODD)ODD: description of the specific operating domain(s) in which an automated function or system is designed to properly operate, including but not limited to roadway types, speed range, environmental conditions (weather, daytime/nighttime, etc.), and other domain constraints Components of the driving task Object and Event Detection and Response (OEDR) Planning Long term Short term Control Lateral control: the task of steering and navigating laterally on the road. Longitudinal control: control the position or velocity of the car (braking and acceleration) along the roadway. Levels of autonomous driving L1: Driving Assistance Adaptive Cruise Control to control speed Lane keeping assistance to help stay in lane L2: Partial Driving Automation L3: Conditional Driving Automation L4: High Driving Automation L5: Full Driving Automation PerceptionPerception: Make sense of the environment and ourselves by identification and understanding motion of objects in the driving environment. Goals of perception Static objects Road and lane markings Curbs Traffic lights Road signs Construction signs, obstructions, and more Dynamic objects Vehicles (4 wheelers and 2 wheelers) Pedestrians Ego localization Position Velocity, acceleration Orientation, angular motion Challenges to perception Robust detection and segmentation: current machine learning and deep learning technologies can not achieve human level capabilities. Sensor uncertainty: Visibility can be challenge GPS measurements may get corrupted LIDAR and Radar data are noisy in terms of their positional value Occlusion, reflection in camera or LIDAR data Illumination, lens flare on sensor that make sensors unavailable。 Weather, precipitation Decision makingPlanning types (window of time) Long term How to navigate from New York to Los Angeles? Short term: Can I change my lane to the lane right of me? Can I pass this intersection and join the left road? Immediate Can I stay on track on this curved road? Accelerate or brake, by how much? Type of planning Reactive planning: rule-based planning. We have rules that take into account the current state of ego and other objects and give decisions. Predictive planning: Make predictions about other vehicles and how they are moving. Then use these predictions to inform our decisions. "},{"title":"carla自定义agent的实现","date":"2020-11-12T23:51:54.000Z","url":"/2020/11/13/carla-agent/","tags":[["carla","/tags/carla/"]],"categories":[["self-driving","/categories/self-driving/"]],"content":"Carla自定义agent在carla0.8.x版本里， 自定义agent通过继承carla.agent.agent.Agent实现。一个简单的ForwardAgent实现如下： 通过重载run_step函数，agent实现了对ego_vehicle的控制。measurements提供了客户端从simulator接收的当前world的状态，比如ego agent的位置，方向等。sensor_data提供了传感器数据如图像，点云等。directions描述了planner提供的high level information。planner里存储了ego vehicle行驶路线中所有关键点的坐标，通过对比ego vehicle当前的位置和前方最近的关键点，planner会计算出high level information包括直行，左转和右转。target提供了路线的终点坐标。run_step最后返回一个VehicleControl对象，通过该对象的三个属性throttle, steer,和brake实现对ego vehicle的控制。 在carla0.9.x版本里，agent接口由scenario_runner/srunner/autoagents/autonomous_agent.py里AutonomousAgent类实现。该类结构如下： __init__函数里定义了_global_plan和_global_plan_world_coord变量用来存储行驶路线中所有关键点的坐标。sensor_interface接收传感器的数据。setup函数初始化自定义agent需要的参数。sensors函数描述自定义agent需要用到的所有传感器的信息。run_step函数实现了自定义agent的控制。其中input_data提供了sensors函数定义的传感器的数据。0.9.x版本的API里，去掉了run_step里的direction参数，允许开发者自己通过global plan和ego vehicle的状态自己写代码去计算方向或者其他需要的量。在0.9.x版本里ForwardAgent的实现如下: 由于API的改变，官方提供的imitation learning和reinforcement learning的代码和模型无法直接在0.9.x版本里运行。"},{"title":"carla安装踩坑记录","date":"2020-11-06T23:18:20.000Z","url":"/2020/11/07/carla-installation/","tags":[["carla","/tags/carla/"]],"categories":[["self-driving","/categories/self-driving/"]],"content":"最近在研究自动驾驶仿真，入坑carla，记录一下学习的过程。这篇文章主要记录了在安装和运行carla时遇到的一些问题和解决方法。之后会学习和分析PythonAPI/examples和scenario_runner里的程序源码。 采用docker方式安装后运行docker run -p 2000-2002:2000-2002 --runtime=nvidia --gpus all carlasim/carla:&lt;version&gt;会报错 sh: 1: xdg-user-dir: not found。在我的电脑上9.10之前的版本不影响正常运行，但9.10版本server端会在报错后退出，目前没有找到解决方案。 采用apt安装或者github release下载安装后，需要配置carla环境变量，解决carla module not found的问题。打开~/.bashrc在文件最后添加： 运行server端./CarUE4.sh时，如果报错X Error of failed request: BadDrawable (invalid Pixmap or Window parameter), 可以用./CarUE4.sh -opengl正常启动程序 运行PythonAPI/examples里的脚本时，确认2中的环境变量已经配置好且python环境里已安装numpy和pygame。如脚本运行时报错pygame.error: unable to make gl context current, 可以尝试卸载并重新安装1.9.6版的pygame。 "},{"title":"categories","date":"2020-11-06T23:16:11.000Z","url":"/categories/index.html","categories":[[" ",""]]},{"title":"search","date":"2020-11-06T23:17:02.000Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"tags","date":"2020-11-06T23:14:35.000Z","url":"/tags/index.html","categories":[[" ",""]]},{"date":"2020-11-13T02:22:09.639Z","url":"/google595976e3bd5331ca.html","categories":[[" ",""]],"content":"google-site-verification: google595976e3bd5331ca.html"}]